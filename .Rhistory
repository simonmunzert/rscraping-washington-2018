head(tweets)
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out it can be quite informative.
twdfm <- dfm(twcorpus, remove=stopwords("english"), remove_url=TRUE,
ngrams=1:2, verbose=TRUE) #
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose=TRUE)
?remove_twitter
?removePunctuation
?remove_punct
?tokens
# split into training and test set
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
# ridge regression
library(glmnet)
require(doMC)
install.packages("glmnet")
source("packages.r")
# ridge regression
registerDoMC(cores=3) # parallel computing on multiple cores using the doMC package
?registerDoMC
options("cores")
detectCores()
# ridge regression
parallel::detectCores()
?cv.glmnet
twdfm[training,]
tweets$engaging[training]
ridge <- cv.glmnet(twdfm[training,], # x matrix
tweets$engaging[training],  # y response
family = "binomial", # family
alpha = 0,
nfolds = 5, # k-folds cross-validation
parallel = TRUE, # enable parallel computing
intercept = TRUE, #
type.measure = "class")
plot(ridge)
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(ridge, twdfm[test,], type="class")
table(preds, tweets$engaging[test]) # confusion matrix
# performance metrics
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
# from the different values of lambda, let's pick the highest one that is
# within one standard error of the best one (why? see "one-standard-error"
# rule -- maximizes parsimony)
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
ridge$lambda
ridge$lambda.1se
summary(ridge)
which(ridge$lambda==ridge$lambda.1se)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
beta
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors=F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
df <- df[order(df$coef, decreasing=TRUE),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# lasso regression
lasso <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 1, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# elastic net
enet <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 0.5, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# note: this will not cross-validate across values of alpha
# computing predicted values
preds <- predict(enet, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
source("packages.r")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
# parameters to explore
tryEta <- c(1,2)
tryDepths <- c(1,2,4)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold=5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
cat("Best model has eta=",bestEta," and depth=", bestDepth, " : ",
bestAcc," accuracy.\n",sep="")
# running best model
rf <- xgboost(data = X[training,],
label = tweets$engaging[training],
max.depth = bestDepth,
eta = bestEta,
nthread = 4,
nround = 1000,
print_every_n=100L,
objective = "binary:logistic")
# out-of-sample accuracy
preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
# feature importance
labels <- dimnames(X)[[2]]
importance <- xgb.importance(labels, model = rf, data=X, label=tweets$engaging)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
head(importance, n=20)
# adding sign
sums <- list()
for (v in 0:1){
sums[[v+1]] <- colSums(X[tweets[,"engaging"]==v,])
}
sums <- do.call(cbind, sums)
sign <- apply(sums, 1, which.max)
df <- data.frame(
Feature = labels,
sign = sign-1,
stringsAsFactors=F)
importance <- merge(importance, df, by="Feature")
## best predictors
for (v in 0:1){
cat("\n\n")
cat("value==", v)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
print(head(importance[importance$sign==v,], n=50))
cat("\n")
cat(paste(unique(head(importance$Feature[importance$sign==v], n=50)), collapse=", "))
}
source("packages.r")
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://google.de")
parsed_doc
parsed_doc <- read_html("https://facebook.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://www.google.com")
parsed_doc
class(parsed_doc)
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://www.google.com")
parsed_doc
Sys.getlocale()
Sys.setlocale("LC_CTYPE", "en_US.UTF-8")
parsed_doc <- read_html("https://www.google.com")
parsed_doc
Sys.setlocale("LC_ALL", "English")
parsed_doc <- read_html("https://www.google.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# sibling
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(.//a)>0]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//*[string-length(text())>50]")
# text-based
html_nodes(parsed_doc, xpath = "//div[@date='October/2011']")
html_nodes(parsed_doc, xpath = "//*[contains(text(), 'magic')]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//div[substring-after(./@date, '/')='2003']//i")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
# 1. go to the following website
browseURL("https://www.jstatsoft.org/about/editorialTeam")
xpath <- "//div[@class='member']/a"
url <- "https://www.jstatsoft.org/about/editorialTeam"
url_parsed <- read_html(url)
source("packages.r")
url_parsed <- read_html(url)
html_nodes(url_parsed, xpath = "//div[@class='member']/a")
url <- "https://www.jstatsoft.org/about/editorialTeam"
url_parsed <- read_html(url)
html_nodes(url_parsed, xpath = "//div[@class = 'member']/a")
url_parsed
html_nodes(url_parsed, xpath = "//div[@class = 'member']//a")
html_nodes(url_parsed, xpath = "//div[@class = 'member']//a") %>% html_text()
xpath <- '//div[@id="content"]//li/a'
html_nodes(url_parsed, xpath = xpath) %>% html_text()
affiliations <- html_nodes(url_parsed, xpath = "//div[@class = 'member']/li") %>% html_text()
affiliations
str_detect(affiliations, "tatisti|athemati") %>% table
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
buildings <- tables[[6]]
buildings
# 6. How many of those buildings are currently built in China? And in which city are most of the tallest buildings currently built?
table(buildings$`Country`) %>% sort
table(buildings$City) %>% sort
buildings$height <- str_extract(buildings$height, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric()
names(buildings)
buildings$height <- str_extract(buildings$`Planned architectural height`, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric()
buildings$height
# What is the sum of heights of all of these buildings?
buildings$height <- str_extract(buildings$`Planned architectural height`, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric() %>% sum
# What is the sum of heights of all of these buildings?
str_extract(buildings$`Planned architectural height`, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric() %>% sum
source("packages.r")
# load RSelenium
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver()
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
# open regions menu
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
regionsElem$clickElement() # click on button
# selection "European Union"
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/ul/li[5]/label/input'
euElem <- remDr$findElement(using = 'xpath', value = xpath)
selectEU <- euElem$clickElement() # click on button
# set time frame
xpath <- '//*[@id="main"]/div/form/div[5]/select[1]'
fromDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
xpath <- '//*[@id="main"]/div/form/div[5]/select[2]'
toDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
xpath <- '//*[@id="main"]/div/form/button[2]'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
resultsPage <- searchElem$clickElement() # click on button
url <- "https://www.starbucks.com/store-locator?map=38.911639,-77.036771,14z&place=washington%20dc"
remDr$navigate(url)
# actions on page
css <- '.filterButton___TxeLm'
click_elem <- remDr$findElement(using = 'css', value = css)
open_elem <- click_elem$clickElement() # click on button
css <- 'li.item___3yAHj:nth-child(6) > button:nth-child(1)'
click_elem <- remDr$findElement(using = 'css', value = css)
open_elem <- click_elem$clickElement() # click on button
css <- '#content > span:nth-child(4) > div > div > div > div.relative > div.invisible.base___3dWsJ.left___N7WEM > div > span > div > button'
click_elem <- remDr$findElement(using = 'css', value = css)
open_elem <- click_elem$clickElement() # click on button
# download page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "starbucks-dc.html")
# import data
content <- read_html("starbucks-dc.html", encoding = "utf8")
store_names <- html_nodes(content, ".truncate") %>% html_text()
store_addresses <- html_nodes(content, ".addressLine___2afjd:nth-child(1)") %>% html_text()
store_names
store_addresses
store_addresses <- html_nodes(content, ".overlayLink___LjA25") %>% html_text()
store_addresses
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.nytimes.com", "search.twitter.com"), .progress=TRUE)
View(ip_df)
# use API
library(rtimes)
load("/Users/simonmunzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
Sys.setenv(NYTIMES_AS_KEY = nytimes_apikey)
terms <- c("John McCain", "Nancy Pelosi", "Bernie Sanders", "Al Franken", "Marco Rubio", "Paul Ryan", "Elizabeth Warren", "Mitch McConnell", "Tim Kaine", "Dianne Feinstein")
nytimes_hits <- numeric()
for(i in seq_along(terms)) {
nytimes_hits[i] <-  as_search(q = terms[i], begin_date = "20170103", end_date = '20180601')$meta$hits
Sys.sleep(runif(1, 1, 2))
}
nytimes_hits_df <- data.frame(name = terms, nytimes_hits, stringsAsFactors = FALSE)
head(nytimes_hits_df)
# API documentation
browseURL("http://ip-api.com/docs/")
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_list <- as_list(ip_parsed)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.nytimes.com") %>% as.data.frame(stringsAsFactors = FALSE)
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
library(pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "2017051500")
head(trump_views)
save(trump_views, file = "trump_pageviews.RData")
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "2017051500")
plot(trump_views$date, trump_views$views, col = "red", type = "l")
lines(clinton_views$date, clinton_views$views, col = "blue")
# 3. familiarize yourself with the OpenWeatherMap API!
browseURL("http://openweathermap.org/current")
# 4. sign up for the API at the address below and obtain an API key!
browseURL("http://openweathermap.org/api")
load("/Users/simonmunzert/rkeys.RDa")
apikey <- paste0("&appid=", openweathermap)
endpoint <- "http://api.openweathermap.org/data/2.5/find?"
city <- "Washington, DC"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
url
city <- "Washington"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
user_df <- lookup_users("RDataCollection")
names(user_df)
user_timeline_df <- get_timeline("RDataCollection")
# add header fields with rvest + httr
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
# don't bombard server
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, names[i]))
Sys.sleep(runif(1, 0, 1))
}
}
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/simonmunzert/GitHub/rscraping-washington-2018")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
