tabs <- html_table(content, fill = TRUE)
tab <- tabs[[1]]
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
remDr$closeServer()
R.Version()$version.string
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com", ))
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
r_text <- get_robotstxt("https://www.google.com/")
r_parsed <- parse_robotstxt(r_text)
names(r_parsed)
table(r_parsed$permissions$useragent, r_parsed$permissions$field)
# use ready-made binding, the aRxiv package
library(aRxiv)
source("packages.r")
# API documentation
browseURL("http://ip-api.com/")
# ipapi package
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
geolocate()
ip_df
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
# use ready-made binding, the aRxiv package
library(aRxiv)
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 200, output_format = "data.frame")
View(arxiv_df)
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 200)
browseURL("http://ip-api.com/")
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
?geolocate
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
names(ip_df)
browseURL("http://arxiv.org/help/api/index")
browseURL("http://arxiv.org/help/api/user-manual")
library(aRxiv)
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 200, output_format = "data.frame")
View(arxiv_df)
query_terms
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
browseURL("http://ip-api.com/docs/")
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
source("packages.r")
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
class(ip_list %>% unlist)
ip_list %>% unlist %>% t
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_parsed %>% as.data.frame(ip_parsed, stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ipapi_grabber <- function(ip = "") {
dat < fromJSON(paste0("http://ip-api.com/json/", ip)) %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("72.33.67.89")
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("72.33.67.89")
?t
a
a <- matrix(1:30, 5, 6)
a
t(a)
167*100*52
browseURL("http://ip-api.com/")
library(ipapi)
?geolocate
c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com")
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
ip_df
View(ip_df)
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
library(ggmap)
geocode("Berlin")
geocode("Berlin")
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
source("packages.r")
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
?t
a <- matrix(1:30, 5, 6)
a
t(a)
foo <- ip_list %>% unlist
class(foo)
foo
ip_list %>% unlist %>% t
foo <- ip_list %>% unlist %>% t
class(foo)
foo <- ip_list %>% unlist
dim(foo)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89")
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("72.33.67.89")
?scale
library("xml2")
library("dplyr")
library("rvest")
library("stringr")
library(stringi)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_unescape_unicode("<\u00b5>")
library(stringr)
library(stringi)
library(rvest)
stri_unescape_unicode("\u00b5")
symbol <- stri_unescape_unicode("\u00b5")
stri_escape_unicode(symbol)
?stri_unescape_unicode
song <- c(
"How many roads must a man walk down
Before you call him a man?
Yes, ’n’ how many seas must a white dove sail
Before she sleeps in the sand?
Yes, ’n’ how many times must the cannonballs fly
Before they’re forever banned?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind
How many years can a mountain exist
Before it’s washed to the sea?
Yes, ’n’ how many years can some people exist
Before they’re allowed to be free?
Yes, ’n’ how many times can a man turn his head
Pretending he just doesn’t see?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind
How many times must a man look up
Before he can see the sky?
Yes, ’n’ how many ears must one man have
Before he can hear people cry?
Yes, ’n’ how many deaths will it take till he knows
That too many people have died?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind")
song
str_replace_all(song, pattern = "[:alpha:]{3,4}", replacement = "")
str_replace(song, pattern = "[:alpha:]{3,4}", replacement = "")
str_replace_all(song, pattern = "man|friend", replacement = "dog")
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog")
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% print()
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
?cat
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
str_c(unlist(str_extract_all(gedichtl, "[[:upper:]]")), collapse="")
str_c(unlist(str_extract_all(song, "[[:upper:]]")), collapse="")
str_replace_all(song, "\\<[[:alpha:]]{1,3}\\>", "bla")
str_replace_all(song, "\\b[:alpha:]{1,3}\\b", "bla")
str_replace_all(song, "\\b[:alpha:]{5,}\\b", "bla")
str_split(song, "\\n")
str_split(song, "\\n") %>% str_trim()
str_split(song, "\\n") %>% sapply(str_trim())
str_split(song, "\\n")
?str_split
str_split(song, "\\n", simplify = TRUE) %>% str_trim()
verses <- str_split(song, "\\n", simplify = TRUE) %>% str_trim()
str_extract(verses, "^[:alpha:]+")
str_extract_all(verses, "^[:alpha:]+|[:alpha:]+.$")
url <- "https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
danger_table <- tables[[2]]
head(danger_table)
danger_table <- tables[[2]]
danger_table <- danger_table[,c(1,3,4,6,7)]
colnames(danger_table) <- c("name","locn","crit","yins","yend")
danger_table$name[1:3]
danger_table$yins
class(danger_table$yins)
danger_table$yend
danger_table$yend
yend_clean <- unlist(str_extract_all(danger_table$yend, "^[[:digit:]]{4}"))
yend_clean
danger_table$yend <- as.numeric(yend_clean)
danger_table$locn[c(1,3,5)]
danger_table$locn[c(1,3,5)]
danger_table$locn[1:5]
country <- str_extract(danger_table$locn, "[[:alpha:] ]+(?=[[:digit:]])") # use forward assertion in Perl regular expression
country
danger_table$locn
country
danger_table$locn
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract(danger_table$locn, reg_y)
y_coords
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2)
y_coords
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2) %>% as.numeric()
danger_table$x_coords <- x_coords
x_coords <- str_extract(danger_table$locn, reg_x) %>% str_sub(3, -1) %>% as.numeric()
danger_table$x_coords <- x_coords
danger_table$locn <- NULL
head(danger_table)
danger_table <- tables[[2]]
danger_table
names(danger_table)
danger_table <- danger_table[,c(1,3,6,7)]
colnames(danger_table) <- c("name","locn","yins","yend")
head(danger_table)
par(oma=c(0,0,0,0))
par(mar=c(0,0,0,0))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
library(maps)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
box()
danger_table$x_coords
# select and rename columns
danger_table <- danger_table[,c(1,3,6,7)]
colnames(danger_table) <- c("name","locn","yins","yend")
# cleanse years
danger_table$yend
yend_clean <- unlist(str_extract_all(danger_table$yend, "^[[:digit:]]{4}"))
danger_table$yend <- as.numeric(yend_clean)
# get countries
danger_table$locn[1:5]
country <- str_extract(danger_table$locn, "[[:alpha:] ]+(?=[[:digit:]])")
country
danger_table$country <- country
# get coordinates
danger_table$locn[1:5]
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2) %>% as.numeric()
danger_table$y_coords <- y_coords
x_coords <- str_extract(danger_table$locn, reg_x) %>% str_sub(3, -1) %>% as.numeric()
danger_table$x_coords <- x_coords
danger_table$locn <- NULL
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = 8)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
box()
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
scale01
vec <- runif(10, 0, 10)
scale01(vec)
df <- mtcars
df[] <- lapply(df, scale01) # or:
df
lapply(df, scale01)
lapply(df, scale01) %>% as.data.frame
df <- iris
df_num <- sapply(df, is.numeric)
df
url <- "https://en.wikipedia.org/wiki/Fourth_Merkel_cabinet"
html_table(url)
read_html(url) %>% html_nodes("//table/tr/td[4]")
read_html(url) %>% html_nodes(xpath = "//table/tr/td[4]")
read_html(url) %>% html_nodes(xpath = "//table/tr/td[4]") %>% html_attr("href")
read_html(url) %>% html_nodes(xpath = "//table/tr/td[4]/a") %>% html_attr("href")
rel_links <- read_html("https://en.wikipedia.org/wiki/Fourth_Merkel_cabinet") %>% html_nodes(xpath = "//table/tr/td[4]/a") %>% html_attr("href")
rel_links
urls <- paste0("https://en.wikipedia.org", rel_links)
urls
dir.create("data/merkelcabinet")
?download.file
urls <- paste0("https://en.wikipedia.org", rel_links)
names <- paste0(basename(urls), ".html")
names
folder <- "data/merkelcabinet"
dir.create(folder)
folder <- "data/merkelcabinet/"
dir.create(folder)
pathnames <- paste0(folder, names)
pathnames
Map(download.file, urls, pathnames)
song <- c(
"How many roads must a man walk down
Before you call him a man?
Yes, ’n’ how many seas must a white dove sail
Before she sleeps in the sand?
Yes, ’n’ how many times must the cannonballs fly
Before they’re forever banned?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind
How many years can a mountain exist
Before it’s washed to the sea?
Yes, ’n’ how many years can some people exist
Before they’re allowed to be free?
Yes, ’n’ how many times can a man turn his head
Pretending he just doesn’t see?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind
How many times must a man look up
Before he can see the sky?
Yes, ’n’ how many ears must one man have
Before he can hear people cry?
Yes, ’n’ how many deaths will it take till he knows
That too many people have died?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind")
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
# load packages
library(stringr)
library(rvest)
library(maps)
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
str_c(unlist(str_extract_all(song, "[[:upper:]]")), collapse="")
str_replace_all(song, "\\b[:alpha:]{5,}\\b", "blah")
verses <- str_split(song, "\\n", simplify = TRUE) %>% str_trim()
str_replace_all(song, "\\b[:alpha:]{5,}\\b", "blah") %>% cat()
verses <- str_split(song, "\\n", simplify = TRUE) %>% str_trim()
str_extract_all(verses, "^[:alpha:]+|[:alpha:]+.$")
# get table
url <- "https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
danger_table <- tables[[2]]
# select and rename columns
danger_table <- danger_table[,c(1,3,6,7)]
colnames(danger_table) <- c("name","locn","yins","yend")
# cleanse years
danger_table$yend
# get table
url <- "https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
danger_table <- tables[[2]]
# select and rename columns
danger_table <- danger_table[,c(1,3,6,7)]
colnames(danger_table) <- c("name","locn","yins","yend")
# cleanse years
danger_table$yend
yend_clean <- unlist(str_extract_all(danger_table$yend, "^[[:digit:]]{4}"))
danger_table$yend <- as.numeric(yend_clean)
# get countries
danger_table$locn[1:5]
country <- str_extract(danger_table$locn, "[[:alpha:] ]+(?=[[:digit:]])")
country
danger_table$country <- country
# get coordinates
danger_table$locn[1:5]
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2) %>% as.numeric()
danger_table$y_coords <- y_coords
x_coords <- str_extract(danger_table$locn, reg_x) %>% str_sub(3, -1) %>% as.numeric()
danger_table$x_coords <- x_coords
danger_table$locn <- NULL
par(oma=c(0,0,0,0))
par(mar=c(0,0,0,0))
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
box()
# 1. Below is a function that scales a vector so it falls in the range [0,1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column of a data frame? Try to come up with solutions using both base R and plyr functions. Use the data.frames mtcars and iris as examples.
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
vec <- runif(10, 0, 10)
scale01(vec)
df <- mtcars
df[] <- lapply(df, scale01) # or:
df <- lapply(df, scale01) %>% as.data.frame
df <- iris
df_num <- sapply(df, is.numeric)
df[df_num] <- lapply(df[df_num], scale01)
## directory ---------------------
wd <- ("./data/ajpsReviewers")
dir.create(wd)
setwd(wd)
## code ----------
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
## step 2: retrieve pdfs
# get page
content <- read_html(url)
# get anchor (<a href=...>) nodes via xpath
anchors <- html_nodes(content, xpath = "//a")
# get value of anchors' href attribute
hrefs <- html_attr(anchors, "href")
# filter links to pdfs
pdfs <- hrefs[ str_detect(basename(hrefs), ".*\\d{4}.*pdf") ]
pdfs
# define names for pdfs on disk
pdf_names <- str_extract(basename(pdfs), "\\d{4}") %>% paste0("reviewers", ., ".pdf")
pdf_names
# download pdfs
for(i in seq_along(pdfs)) {
download.file(pdfs[i], pdf_names[i], mode="wb")
}
## step 3: import pdf
rev_raw <- pdf_text("reviewers2015.pdf")
library(pdftools)
## step 3: import pdf
rev_raw <- pdf_text("reviewers2015.pdf")
class(rev_raw)
rev_raw[1]
rev_raw
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
rev_all
rev_raw[1]
surname <- str_extract(rev_all, "[[:alpha:]-]+")
prename <- str_extract(rev_all, " [.[:alpha:]]+")
surname
prename
rev_df <- data.frame(raw = rev_all, surname = surname, prename = prename, stringsAsFactors = F)
View(rev_df)
View(rev_df)
rev_df <- data.frame(raw = rev_all, surname = surname, prename = prename, stringsAsFactors = F)
rev_df$institution <- NA
for(i in 1:nrow(rev_df)) {
rev_df$institution[i] <- rev_df$raw[i] %>% str_replace(rev_df$surname[i], "") %>% str_replace(rev_df$prename[i], "") %>% str_trim()
}
rev_df <- rev_df[-c(1,2),]
rev_df <- rev_df[!is.na(rev_df$surname),]
head(rev_df)
## step 5: geocode reviewers/institutions
# geocoding takes a while -> save results
# 2500 requests allowed per day
pos <- data.frame(lon = NA, lat = NA)
unique_institutions <- unique(rev_df$institution)
unique_institutions <- unique_institutions[!is.na(unique_institutions)]
if (!file.exists("institutions2015_geo.RData")){
for (i in 1:length(unique_institutions)) {
pos[i,] <- geocode(unique_institutions[i], source = "google", force = "FALSE")
}
pos$institution <- unique_institutions
save(pos, file="institutions2015_geo.RData")
} else {
load("institutions2015_geo.RData")
}
head(pos)
rev_geo <- merge(rev_df, pos, by = "institution", all = T)
## step 6: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev_geo$lon, y=rev_geo$lat) ,
color="#F54B1A90", size=1,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-70,80))
map
library(ggmap)
## step 6: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev_geo$lon, y=rev_geo$lat) ,
color="#F54B1A90", size=1,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-70,80))
map
## step 7: plot reviewers, Germany
mapGermany <- get_map(location = 'Germany', zoom = 6)
## step 7: plot reviewers, Germany
mapGermany <- get_map(location = 'Germany', zoom = 6)
## step 7: plot reviewers, Germany
mapGermany <- get_map(location = 'Germany', zoom = 6)
## step 7: plot reviewers, Germany
mapGermany <- get_map(location = 'Germany', zoom = 6)
map <-
ggmap(mapGermany) +
geom_point(data = rev_geo, aes(x= lon, y = lat) ,
color="#F54B1A90", size = 1,
na.rm=T)
map
