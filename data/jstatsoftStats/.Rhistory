type.measure = "class")
plot(ridge)
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(ridge, twdfm[test,], type="class")
table(preds, tweets$engaging[test]) # confusion matrix
# performance metrics
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
# from the different values of lambda, let's pick the highest one that is
# within one standard error of the best one (why? see "one-standard-error"
# rule -- maximizes parsimony)
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
ridge$lambda
ridge$lambda.1se
summary(ridge)
which(ridge$lambda==ridge$lambda.1se)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
beta
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors=F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
df <- df[order(df$coef, decreasing=TRUE),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# lasso regression
lasso <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 1, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# elastic net
enet <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 0.5, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# note: this will not cross-validate across values of alpha
# computing predicted values
preds <- predict(enet, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
source("packages.r")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
# parameters to explore
tryEta <- c(1,2)
tryDepths <- c(1,2,4)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold=5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
cat("Best model has eta=",bestEta," and depth=", bestDepth, " : ",
bestAcc," accuracy.\n",sep="")
# running best model
rf <- xgboost(data = X[training,],
label = tweets$engaging[training],
max.depth = bestDepth,
eta = bestEta,
nthread = 4,
nround = 1000,
print_every_n=100L,
objective = "binary:logistic")
# out-of-sample accuracy
preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
# feature importance
labels <- dimnames(X)[[2]]
importance <- xgb.importance(labels, model = rf, data=X, label=tweets$engaging)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
head(importance, n=20)
# adding sign
sums <- list()
for (v in 0:1){
sums[[v+1]] <- colSums(X[tweets[,"engaging"]==v,])
}
sums <- do.call(cbind, sums)
sign <- apply(sums, 1, which.max)
df <- data.frame(
Feature = labels,
sign = sign-1,
stringsAsFactors=F)
importance <- merge(importance, df, by="Feature")
## best predictors
for (v in 0:1){
cat("\n\n")
cat("value==", v)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
print(head(importance[importance$sign==v,], n=50))
cat("\n")
cat(paste(unique(head(importance$Feature[importance$sign==v], n=50)), collapse=", "))
}
source("packages.r")
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://google.de")
parsed_doc
parsed_doc <- read_html("https://facebook.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://www.google.com")
parsed_doc
class(parsed_doc)
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://www.google.com")
parsed_doc
Sys.getlocale()
Sys.setlocale("LC_CTYPE", "en_US.UTF-8")
parsed_doc <- read_html("https://www.google.com")
parsed_doc
Sys.setlocale("LC_ALL", "English")
parsed_doc <- read_html("https://www.google.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# sibling
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(.//a)>0]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//*[string-length(text())>50]")
# text-based
html_nodes(parsed_doc, xpath = "//div[@date='October/2011']")
html_nodes(parsed_doc, xpath = "//*[contains(text(), 'magic')]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//div[substring-after(./@date, '/')='2003']//i")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
source("packages.r")
# 1. specify URL
url <- "https://www.nytimes.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
headings_nodes
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'balancedHeadline']")
headings_nodes
headings_nodes
# 1. specify URL
url <- "https://www.nytimes.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'balancedHeadline']")
headings_nodes
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'css-8uvv5f']")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
headings_nodes <- html_nodes(url_parsed, css = ".css-8uvv5f , .balancedHeadline")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
headings_nodes <- html_nodes(url_parsed, css = ".balancedHeadline")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
headings_nodes <- html_nodes(url_parsed, css = ".css-8uvv5f , .balancedHeadline")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
url <- "https://www.washingtonpost.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'text-align-inherit']")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'text-align-inherit']")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
url_parsed
# 1. specify URL
url <- "https://www.nytimes.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "esl82me2", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "balancedHeadline", " " ))]'
headings_nodes <- html_nodes(url_parsed, xpath = xpath)
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
xpath <- '//*[contains(concat( " ", "esl82me2", " " ))] | //*[contains(concat( " ", "balancedHeadline", " " ))]'
headings_nodes <- html_nodes(url_parsed, xpath = xpath)
xpath <- '//*[contains(concat( " ", "esl82me2", " " ))] | //*[contains(concat( " ", "balancedHeadline", " " ))]'
headings_nodes <- html_nodes(url_parsed, xpath = xpath)
url <- "https://www.washingtonpost.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
headings
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
headings_nodes <- html_nodes(url_parsed, xpath = xpath)
url <- "https://www.washingtonpost.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
headings_nodes <- html_nodes(url_parsed, xpath = xpath)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[@class = "text-align-inherit")]')
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[@class = "text-align-inherit"]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
url <- "https://www.washingtonpost.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[@class = "text-align-inherit"]')
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings_nodes <- html_nodes(url_parsed, xpath = '//div[@class = "headline']/a)
headings_nodes <- html_nodes(url_parsed, xpath = '//div[@class = "headline"]/a')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings_nodes <- html_nodes(url_parsed, xpath = '//div[@class = "headline"]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
headings_nodes <- html_nodes(url_parsed, xpath = '//div[@class = contains("headline")]')
headings_nodes <- html_nodes(url_parsed, xpath = '//div[contains("headline")]')
?contains
?concat
headings_nodes <- html_nodes(url_parsed, xpath = '//div/[contains("headline")]')
headings_nodes <- html_nodes(url_parsed, xpath = '//div/[@id = contains("headline")]')
headings_nodes <- html_nodes(url_parsed, xpath = '//div[@id = contains("headline")]')
headings_nodes <- html_nodes(url_parsed, xpath = '//*[@id = contains("headline")]')
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[@class = "text-align-inherit"]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
# 1. specify URL
url <- "https://www.washingtonpost.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
headings_nodes <- html_nodes(url_parsed, xpath = '//*[contains(@class, "text-align-inherit")')
headings_nodes <- html_nodes(url_parsed, xpath = "//*[contains(contains(concat(' ', @class, ' '), 'text-align-inherit'')")
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
# 5. tidy headlines
headings <- str_subset(headings, "^[:alnum:]")
headings
head(headings)
length(headings)
## scraping HTML tables with rvest
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
tables <- html_table(url_p, header = TRUE, fill = TRUE)
mps <- tables[[4]]
head(mps)
names(mps) <- c("constituency", "mp", "party")
mps <- mps[2:nrow(mps),]
mps <- filter(mps, !str_detect(constituency, fixed("[edit]")))
table(mps$party, str_detect(mps$mp, "^Sir ")) # how many "Sirs" per party?
# to learn about it, visit
vignette("selectorgadget")
# to install it, visit
browseURL("http://selectorgadget.com/")
# 3. extract specific nodes with XPath
nodes <- html_nodes(url_parsed, xpath = '//td[2]/a[1]')
# 4. extract content from nodes
article_links <- html_text(nodes)
article_links
# 1. specify URL
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_Washington,_D.C."
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
nodes <- html_nodes(url_parsed, xpath = '//td[2]/a[1]')
nodes
# 4. extract content from nodes
article_links <- html_text(nodes)
article_links
# 3. extract specific nodes with XPath
nodes <- html_nodes(url_parsed, xpath = '//td[2]/a[1]')
# 4. extract content from nodes
article_links <- html_text(nodes)
article_links
head(article_links)
length(article_links)
# set temporary working directory
tempwd <- ("../data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
browseURL("http://www.jstatsoft.org/")
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,85,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
issurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", issurl)
names <- paste0(rep(volurl, each = 9), "_", issurl, ".html")
# check success
list_files <- list.files(folder, pattern = "0.*")
# download pages
folder <- "html_articles/"
# check success
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
# construct data frame
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish, stringsAsFactors = FALSE)
head(dat)
# plot download statistics
dattop <- dat[order(dat$numViews, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numViews)
plot(density(dat$numViews, from = 0), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JStatSoft")
