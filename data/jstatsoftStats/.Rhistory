## peparations -------------------
source("packages.r")
## construct a document-feature matrix (DFM) ---------------------
# dfm() constructs a document-feature matrix (DFM) from a tokens object
irish_toks <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
summary(irish_toks)
irish_dfm <- dfm(irish_toks)
irish_dfm
?dfm
# dfm has many useful options
irish_dfm <- dfm(data_corpus_irishbudget2010,
tolower = TRUE,
remove_punct = TRUE,
stem = TRUE,
ngrams = 1:3
)
irish_dfm
?tokens
?tokenize
?char_wordstem
# clean up
tokens_wordstem(immig_corp)
# words
immig_corp <- corpus(data_char_ukimmig2010)
toks <- tokens(immig_corp, what = "word")
summary(toks)
toks$Labour[1:100]
# sentences
toks <- tokens(immig_corp, what = "sentence")
toks$Labour[1:10]
# clean up
toks <- tokens(immig_corp, what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_hyphens = TRUE)
toks$Labour[1:10]
# clean up
tokens_wordstem(immig_corp)
toks$Labour[1:10]
# clean up
tokens_wordstem(toks$Labour[1:10])
# clean up
tokens_wordstem(tokenize(toks$Labour[1:10]))
# clean up
tokens_wordstem(tokens(toks$Labour[1:10]))
tokens(toks$Labour[1:10])
# clean up
tokens_wordstem(tokens(toks$Labour[1:10]))
toks$Labour
tokenize(toks$Labour)
# clean up
tokens_wordstem(tokens(toks$Labour[1:10]))
# clean up
tokens_wordstem(tokens(toks$Labour[1:10])) %>% unlist
char_wordstem(tokens(toks$Labour[1:10])) %>% unlist
char_wordstem(tokenize(toks$Labour[1:10])) %>% unlist
tokens_wordstem(tokens("Die Münchner essen ihre Weißwurst gerne vor Mittag")) %>% unlist
tokens_wordstem(tokens("Die Münchner essen ihre Weißwurst gerne vor Mittag"), language = "de") %>% unlist
tokens_wordstem(tokens("Die Münchner essen ihre Weißwurst gerne vor Mittag"), language = "en") %>% unlist
tokens_wordstem(tokens("Die Münchner essen ihre Weißwurst gerne vor Mittag"), language = "de") %>% unlist
tokens_wordstem(tokens("Die Münchner essen ihre Weißwürste gerne vor Mittag"), language = "de") %>% unlist
stopwords("english")
# dealing with stop words
stopwords("english")
?stopwords
stopwords("de")
stopwords("german")
?dfm
irish_dfm <- dfm(data_corpus_irishbudget2010,
remove_punct = TRUE,
remove=c(stopwords("english"), "£"), verbose=TRUE)
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# navigational operators
html_nodes(parsed_doc, xpath = "//title/..")
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# sibling
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[position()=1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div/p[last()-1]")
html_nodes(parsed_doc, xpath = "//div[count(.//a)>0]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//*[string-length(text())>50]")
html_nodes(parsed_doc, xpath = "//div[not(count(./@*)>2)]")
# text-based
html_nodes(parsed_doc, xpath = "//div[@date='October/2011']")
html_nodes(parsed_doc, xpath = "//*[contains(text(), 'magic')]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//div[substring-after(./@date, '/')='2003']//i")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 1. specify URL
url <- "https://www.nytimes.com"
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", "") %>% str_trim()
head(headings)
length(headings)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
# 4. extract content from nodes
headings <- html_text(headings_nodes)
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
length(headings)
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_human_spaceflights")
tables <- html_table(url_p, header = TRUE, fill = TRUE)
spaceflights <- tables[[1]]
spaceflights
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
tables <- html_table(url_p, header = TRUE, fill = TRUE)
mps <- tables[[4]]
mps
head(mps)
View(mps)
names(mps) <- c("constituency", "mp", "party")
head(mps)
mps[1,] <- NULL
mps[1,]
mps <- mps[2:nrow(mps),]
head(mps)
nrow(mps)
mps <- filter(mps, !str_detect(fixed("[edit]")))
nrow(mps)
mps <- filter(mps, !str_detect(constituency, fixed("[edit]")))
View(mps)
str_detect(mps$mp, "^Sir ")
table(str_detect(mps$mp, "^Sir "), mps$party)
table(mps$party, str_detect(mps$mp, "^Sir "))
## SelectorGadget is magic. Proof:
browseURL("https://www.nytimes.com")
url <- "https://www.nytimes.com"
xpath <-  '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
url_parsed <- read_html(url)
html_nodes(url_parsed, xpath = xpath) %>% html_text()
## example: fetching and analyzing jstatsoft download statistics
# set temporary working directory
tempwd <- ("../data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,83,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
issurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", issurl)
names <- paste0(rep(volurl, each = 9), "_", issurl, ".html")
browseURL("http://www.jstatsoft.org/")
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,85,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
issurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", issurl)
names <- paste0(rep(volurl, each = 9), "_", issurl, ".html")
# download pages
folder <- "html_articles/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, names[i])) # , method = "libcurl" might be needed on windows machine
Sys.sleep(runif(1, 0, 1))
}
}
# check success
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
# delete non-existing articles
files_size <- sapply(list_files_path, file.size)
table(files_size) %>% sort()
delete_files <- list_files_path[files_size == 25566]
sapply(delete_files, file.remove)
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
# construct data frame
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish, stringsAsFactors = FALSE)
head(dat)
# plot download statistics
dattop <- dat[order(dat$numViews, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numViews)
plot(density(dat$numViews, from = 0), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JStatSoft")
